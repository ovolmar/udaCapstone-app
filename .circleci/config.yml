version: 2.1
commands:
  destroy-eks-env:
    description: Destroy EKS CF Stack
    parameters:
      workflowid:
        type: string
        default: ${ CIRCLE_WORKFLOW_ID:0:7 }
    steps:
      - run:
          name: delete eks cluster
          when: on_fail
          command: |
            curl --silent --location https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_Linux_amd64.tar.gz | tar -xz -C /tmp
            sudo mv /tmp/eksctl /usr/local/bin
            eksctl delete cluster -f infrastructure/k8s-cluster.yml
  cluster-status:
    description: Get cluster status 
    parameters:
      workflowid:
        type: string
        default: ${CIRCLE_WORKFLOW_ID:0:7}
    steps:
      - run:
          name: cluster status
          command: |
            /usr/local/bin/aws eks describe-cluster \
            --region {AWS_DEFAULT_REGION}  \
            --name udacapstone-k8s-cluster \
            --query "cluster.status"

# Adding a build command here with a revert-migrations
#
#--------------------------------------------------------------------------------------------------------------------------          
jobs:
  lint:
    docker:
      - image: python:3.7.3-stretch
    steps:
      - checkout 
      - restore_cache:
          keys:
            - v1-dependencies-{{ checksum "requirements.txt" }}
            # fallback to using the latest cache if no exact match is found
            - v1-dependencies-
      - run:
          name: "linting"
          command: |
            python3 -m venv venv
            . venv/bin/activate
            make install
            # Install hadolint
            wget -O /bin/hadolint https://github.com/hadolint/hadolint/releases/download/v1.16.3/hadolint-Linux-x86_64 &&\
            chmod +x /bin/hadolint
            make lint
      - save_cache:
          paths:
            - ./venv
          key: v1-dependencies-{{ checksum "requirements.txt" }}

  buildcontainer:
    docker:
      # Use the same Docker base as the project
      - image: cimg/go:1.17
        auth:
          username: $DOCKER_LOGIN
          password: $DOCKER_PASSWORD
    steps:
      - checkout
      - restore_cache:
          keys:
            - v1-dependencies-{{ checksum "requirements.txt" }}
            # fallback to using the latest cache if no exact match is found
            - v1-dependencies-
      - setup_remote_docker:
          version: 20.10.14
          docker_layer_caching: true
      - run:
          name: "Building Docker Container and upload it"
          command: |
            touch ".env"
            echo DOCKER_LOGIN=${DOCKER_LOGIN} >> ".env"
            echo DOCKER_PASSWORD=${DOCKER_PASSWORD}  >> ".env"
            echo IMAGE_NAME=${IMAGE_NAME} >> ".env"
            source ".env"
            docker build -t ${IMAGE_NAME} .
            docker tag $IMAGE_NAME ovolmar/${IMAGE_NAME}:latest
            docker login -u ${DOCKER_LOGIN} -p ${DOCKER_PASSWORD}
            docker push ovolmar/${IMAGE_NAME}
      - save_cache:
          paths:
            - ./venv
          key: v1-dependencies-{{ checksum "requirements.txt" }}

  deploycluster:
    docker:
      - image: cimg/aws:2022.06
    working_directory: ~/tmp
    steps:
      - checkout

      # - run:
      #     name: Install authenticator
      #     command: |
      #       cd /tmp/
      #       wget https://github.com/kubernetes-sigs/aws-iam-authenticator/releases/download/v0.5.9/aws-iam-authenticator_0.5.9_linux_amd64 -o aws-iam-authenticator
      #       sudo chmod +x /tmp/aws-iam-authenticator
      #       sudo mv /tmp/aws-iam-authenticator /usr/local/bin
      - run:
          name: Loading Latest AWSCLI
          command: |
            if [ -f /usr/local/bin/aws ]; then
              echo "AWS is loaded moving on"
            else 
              curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
              unzip awscliv2.zip
              sudo ./aws/install
            fi 

      - run:
          name: Install kubectl and eksctl
          command: |
            aws --version
            curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
            sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
            curl --silent --location https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_Linux_amd64.tar.gz | tar -xz -C /tmp
            sudo mv /tmp/eksctl /usr/local/bin

      - run: 
          name: Build EKS cluster
          command: |
            /usr/local/bin/aws eks describe-cluster --region ${AWS_DEFAULT_REGION} --name udacapstone-k8s-cluster --query "cluster.status" 2>/dev/null
            #eksctl create cluster -f infrastructure/k8s-cluster.yml
            if [ $? != 0 ]; then  eksctl create cluster -f infrastructure/k8s-cluster.yml ; else exit 0 ; fi
  pod-launcher:
    docker:
      - image: cimg/aws:2022.06
    steps:
      - checkout 
      - run:
          name: launching pods
          command: |
            /usr/local/bin/aws eks update-kubeconfig --name udacapstone-k8s-cluster --region ${AWS_DEFAULT_REGION} 
            kubectl apply -f infrastructure/bluepod.yml
            kubectl apply -f infrastructure/blue-services.yml
            kubectl apply -f infrastructure/ingress.yml
            kubectl expose deployment flask-blue --type=ClusterIP --name=service-cluser-ip
            sleep 40
            kubectl get svc



      # - run:
      #     name: get cluster credz
      #     command: |
      #       mkdir ~/tmp/myArtifacts
      #       cp ~/.kube/config ~/tmp/myArtifacts/ 
      #       eksctl utils write-kubeconfig --cluster=udacapstone-k8s-cluster --kubeconfig=~/kube-cred.log --set-kubeconfig-context=true
      #       cp ~/kube-cred.log ~/tmp/myArtifacts/
   
      # - persist_to_workspace:
      #     root: ~/
      #     paths:
      #       - kube-cred.log
      #       - myArtifacts/config
      # - store_artifacts:
      #     path: ~/tmp/myArtifacts


      # - run: 
      #     name: Authentication Handshake
      #     command: |
      #       aws eks --region ${AWS_DEFAULT_REGION} update-kubeconfig --name udacapstone-k8s-cluster
      # - run: 
      #     name: Exposing application 
      #     command: |
      #       kubectl apply -f infrastructure/weather-pod.yml
      #       kubectl expose deployment capstone-load-balancer --type=LoadBalancer --name=capstone-weather
            

workflows: 
  default:
    jobs:
      - buildcontainer
      - lint
      - deploycluster:
          requires: [buildcontainer]
      - pod-launcher:
          requires: [deploycluster]